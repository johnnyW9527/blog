#### 2 运行架构

##### 2.1 部署模式

###### 2.1.1 相关配置

*  核心目录

```
bin：启动脚本
conf：配置文件
examples：样例程序
lib：jar存放地
log：日志存放地
```

* bin目录

```
flink：核心运行job
historyserver.sh：历史服务器启动停止脚本
start-cluster.sh：启动集群脚本
stop-cluster.sh：停止集群脚本
yarn-session.sh：运行job的一种模式
```

* conf目录

```
flink-conf.yaml：核心配置
masters：配置jobManager主机
slaves：配置taskManager主机
log4j.properties：配置日志
```

* flink-conf.yaml文件

```
# jobManager 服务主机
jobmanager.rpc.address: localhost
# jobManager 服务端口
jobmanager.rpc.port: 6123
# jobManager 堆大小 默认 1G
jobmanager.heap.size: 1024m
# taskManager 处理内存大小 包括堆内内存和堆外内存
taskmanager.memory.process.size: 1728m
# taskManager 的 slots 个数 默认1
taskmanager.numberOfTaskSlots: 1
# 并行度大小 并行度大小少于等于总共的 slots（taskManager 个数 * slots 个数）
parallelism.default: 1
# 前端页面端口
#rest.port: 8081
```

###### 2.1.2 Yarn部署模式

* Session-cluster模式

```
Session-Cluster 模式需要先启动集群，然后再提交作业，接着会向 yarn 申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn 中的其中一个作业执行完成后，释放了资源，下个作业才会正常提交。所有作业共享 Dispatcher 和 ResourceManager；共享资源；适合规模小执行时间短的作业。
```

```
bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm yarn-deploy-wordcount -d
```

*-n(–container)：TaskManager 的数量。*
*-s(–slots)： 每个 TaskManager 的 slot 数量，默认一个 slot 一个 core，默认每个taskmanager 的 slot 的个数为 1，有时可以多一些 taskmanager，做冗余。*
*-jm：JobManager 的内存（单位 MB)。*
*-tm：每个 taskmanager 的内存（单位 MB)。*
*-nm：yarn 的 appName(现在 yarn 的 ui 上的名字)。*
*-d：后台执行*

停止job

```
yarn application --kill application_1615631116563_0001 
```

* pre job cluster

```
一个Job 会对应一个集群，每提交一个作业会根据自身的情况，都会单独向 yarn申请资源，直到作业执行完成，一个作业的失败与否并不会影响下一个作业的正常提交和运行。独享 Dispatcher 和 ResourceManager，按需接受资源申请；适合规模大长时间运行的作业。
每次提交都会创建一个新的 flink 集群，任务之间互相独立，互不影响，方便管理。任务执行完成之后创建的集群也会消失
```

```
bin/flink run -m yarn-cluster -c com.tan.flink.deploy.DeployWordcount -p 2 lib/flink-1.0-SNAPSHOT.jar --host 192.168.200.102 --port 9999
```

##### 2.2 运行组件

###### 2.2.1 作业管理器（jobManager）

**控制一个应用程序执行的主进程，是Flink集群中任务管理和调度的核心**

jobMaster

* 是JobManager中最核心的组件，负责处理单独的作业（Job）
* 在提交作业时，JobMaster会先接受到要执行的应用，一般是由客户端提交来的（包括Jar包，数据流图（dataflow graph），和作业图（Job Graph））
* JobMaster会把JobGraph转换成一个物理层面的数据流图，这个图叫做“执行图”（Execution Graph），它包含了所有可以并发执行的任务。JobMaster会向资源管理器（Resource Manager）发出请求，申请执行任务必要的资源。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。
* 在运行过程中，JobMaster会负责所有需要中央协调的操作，比如检查点（Checkpoints）的协调

###### 2.2.2 任务管理器（taskmanager）

```
link中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行。在执行过程中，一个TaskManager可以跟其它运行同一应用程序TaskManager交换数据
```

###### 2.2.3 资源管理器（resourceManager）

```
主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger 插槽是Flink中定义的处理资源单元。
Flink为不同的环境和资源管理工具提供了不同资源管理器，比如YARN、Mesos、K8s，以及standalone部署。
当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器。
该资源管理器不同于YARN的ResourceManager
```

###### 2.2.4 分发器（dispatcher）

```
可以跨作业运行，它为应用提交提供了REST接口。
当一个应用被提交执行时，分发器就会启动并将应用移交给一个JobManager。
Dispatcher也会启动一个Web UI，用来方便地展示和监控作业执行的信息。
Dispatcher在架构中可能并不是必需的，这取决于应用提交运行的方式
```

##### 2.3 任务提交流程

* 整体构成

![](C:\study\mlog\picture\49.png)

* Yarn会话流程

![](C:\study\mlog\picture\50.png)

* Yarn单作业流程

![](C:\study\mlog\picture\51.png)

##### 2.4 基础概念

###### 2.4.1 程序与数据流（DataFlow）

* 所有的Flink程序都是由三部分组成: Source Transformation 和 Sink（输入、转换、输出）
* Source负责读取数据源，Transformation利用各种算子进行处理加工，Sink负责输出
* 在运行时，Flink上运行的程序会被映射成“逻辑数据流”（Dataflows），它包含了这三部分
* 每一个dataflow以一个或多个Source开始以一个或多个Sinks结束，dataflow类似于任意的有向无环图（DAG）
* 在大部分情况下，程序中的转换运算（transformation）跟dataflow中的算子（operator）是一一对应的关系

###### 2.4.2 并行度

* 每一个算子（Operator）可以包含一个或多个子任务（Operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中完全独立地执行
* 一个特定算子的子任务（subtask）的个数被称之为并行度（parallelism）

**不提倡设置全局并发度，推荐每个算子中定义并行度**

###### 2.4.3 算子链（Operator Chain）

**数据传输形式**

* 一个程序中，不同的算子可能具有不同的并行度
* 算子之间传输数据的形式可以是one-to-one（forwarding 直传 ）的模式也可以是redistributing（重新分配）的模式，具体是哪一种形式，取决于算子的种类

```
One-to-One:Stream维护着分区以及元素的顺序（比如source和map之间）。这意味着map算子的子任务看到的元素个数以及顺序根跟source算子的子任务生产的元素的个数、顺序相同。map、filter、flatMap等算子都是one-to-one的对应关系
```

```
Redistributing： Stream的分区会发生改变。每一个算子的子任务依据所算子的transformation发送数据到不同的目标任务。例如，keyby基于hashcode重分区，而broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程类似于Spark的shuffle过程；
```

**算子链**

* Flink采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发(Local forward)的方式进行连接
* 相同并行度的one-to-one操作。Flink这些相连的算子链接在一起形成一个task,原来的算子成为里面的subtask
* 并行度相同，并且是one-to-one操作，两个条件缺一不可

###### 2.4.4 执行图（ExecutionGraph）和作业图(JobGraph)

Fink中的执行图可以分成四层：StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图

* StreamGraph：是根据用户通过StreamAPI编写的代码生成的最初的图。用来表示程序的拓扑结构
* JobGraph： StreamGraph经过优化后生成了JobGraph。提交给JobManager的数据结构，主要的优化为，将多个符合条件的节点chain在一起作为一个节点
* ExecutionGraph:JobManager根据JobGraph生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构
* 物理执行图：JobManager根据ExecutionGraph对Job进行调度后，在各个TaskManager上部署Task后形成的“图”，并不是一个具体的数据结构

###### 2.4.5 任务和任务槽

* Flink中每一个TaskManager都是一个JVM进程，它可能会在独立的线程上执行一个或多个子任务
* 为了控制一个TaskManger能接收多少个Task，TaskManager通过Task Slot来进行控制（一个TaskManager至少有一个Slot）

**任务共享slot**

* 默认情况下，Flink允许子任务共享slot，这样的结果是，一个Slot可以保存作业的整个管道
* 当我们将资源密集型和非密集型的任务同时放到slot中，它们就可以自行分配对资源占用的比例，从而保证最重的活平均分配给所有的TaskManager

**slot和并行度**

* 静态概念,是指TaskManager具有并发执行能力
* 通过参数taskmanager.numberOfTaskSlots进行配置（通常推荐配置为机器CPU核数
* 动态概念，也就是TaskManager运行程序时实际使用的并发能力
* 通过参数parallelism.default进行配置

