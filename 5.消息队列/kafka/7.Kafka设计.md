#### 7.Kafka设计

##### 7.1 broker副本与ISR设计

> kafka把分区的所有副本均匀地分配到所有broker上，并从这些副本中挑选一个作为leader副本对外提供服务，而其他副本被称为follower副本，只能被动地向leader副本请求数据，从而保持与leader副本的同步;
>
> 所谓isr，就是Kafka集群动态维护的一组同步副本集合，每个topic分区都有自己的isr列表，isr中的所有副本都与leader保持同步状态，而producer写入的一条Kafka 消息只有被isr中的所有副本都接收到，才被视为“已提交”状态，由此可见，若isr中有n个副本，那么该分区最多可以忍受n-1个副本崩溃而不丢失已提交消息。
>
> 1. follower副本同步:follower副本只做一件事情：向leader副本请求数据
> 2. 如何界定同步
>
> * 9.0.0版本之前：落后消息数+时间
> * 9.0.0版本之后：由于慢以及进程卡壳导致的滞后–即follower副本落后leader副本的时间间隔，replica.lag.time.max.ms默认是10秒如果一个follower副本落后leader的时间持续性地超过了这个参数值，那么该follower副本就是不同步的

##### 7.2 Kafka Consumer 重置offset

> 在Kafka Version为0.11.0.0之后，Consumer的Offset信息不再默认保存在Zookeeper上，而是选择用Topic的形式保存下来。在命令行中可以使用kafka-consumer-groups的脚本实现Offset的相关操作
>
> 更新offset的三个维度：
>
> * Topic的作用域
> * 重置策略
> * 执行方案

###### 7.2.1 Topic作用域

* --all-topics：为consumer group下所有topic的所有分区调整位移
* --topic t1 --topic t2：为指定的若干个topic的所有分区调整位移
* --topic t1:0,1,2：为指定的topic分区调整位移

###### 7.2.2 重置策略

* --to-earliest：把位移调整到分区当前最小位移
* --to-latest：把位移调整到分区当前最新位移
* --to-current：把位移调整到分区当前位移
* --to-offset  <offset>： 把位移调整到指定位移处
* --shift-by N： 把位移调整到当前位移 + N处，注意N可以是负数，表示向前移动
* -to-datetime <datetime>：把位移调整到大于给定时间的最早位移处，datetime格式是yyyy-MM-ddTHH:mm:ss.xxx，比如2017-08-04T00:00:00.000
* --by-duration <duration>：把位移调整到距离当前时间指定间隔的位移处，duration格式是PnDTnHnMnS，比如PT0H5M0S
* --from-file <file>：从CSV文件中读取调整策略

###### 7.2.3 执行方案

> 什么参数都不加：只是打印出位移调整方案，不具体执行
>
> * --execute：执行真正的位移调整
> * --export：把位移调整方案按照CSV格式打印，方便用户成csv文件，供后续直接使用

###### 7.2.4 注意事项

> 1. consumergroup状态必须是inactive的，即不能是处于正在工作中的状态；
> 2. 不加执行方案，默认是只做打印操作；

##### 7.3 常用示例

###### 7.3.1 更新到当前group最初的offset位置

```
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-earliest --execute
```

###### 7.3.2 更新到指定的offset位置

```
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-offset 500000 --execute
```

###### 7.3.3 更新到当前offset位置（解决offset的异常）

```
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-current --execute
```

###### 7.3.4 offset位置按设置的值进行位移

```
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --shift-by -100000 --execute
```

###### 7.3.5offset设置到指定时刻开始

```
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-datetime 2017-08-04T14:30:00.000
```

##### 7.4 常用语法

1. 创建topic

```
./kafka-topics.sh --create --topic test1 --replication-factor 2 --partitions 3 --zookeeper hbp001:2181
```

2. 增加partition

```
./kafka-topics.sh --zookeeper node01:2181 --alter --topic t_cdr --partitions 10
```

3. 查看所有topic列表

```
./kafka-topics.sh --zookeeper hbp201:2181 –list
```

4. 查看指定topic信息

```
./kafka-topics.sh --zookeeper hbp201:2181 --describe --topic t_cdr
```

5. 控制台向topic生产数据

```
./kafka-console-producer.sh --broker-list node86:9092 --topic t_cdr
```

6. 控制台消费topic的数据

```
./kafka-console-consumer.sh -zookeeper hdh247:2181 --from-beginning --topic fieldcompact
```

7. 查看topic某分区偏移量最大（小）值

```
./kafka-run-class.sh kafka.tools.GetOffsetShell --topic hive-mdatabase-hostsltable --time -1 --broker-list node86:9092 --partitions 0
注：time为-1时表示最大值，time为-2时表示最小值
```

8. 增加topic分区数

```
为topic t_cdr 增加到10个分区
./kafka-topics.sh --zookeeper hbp201:2181 --alter --topic t_cdr --partitions 10
```

9. 删除topic

```
慎用，只会删除zookeeper中的元数据，消息文件须手动删除
./kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper hbp201:2181 --topic t_cdr
```

10. 查看consumer组内消费的offset

```
./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test --topic testKJ1
./kafka-consumer-offset-checker.sh --zookeeper 192.168.0.201:2181 --group group1 --topic group1
```

11. 查看kafka某分区日志具体内容

```
./kafka-run-class.sh kafka.tools.DumpLogSegments -files /tmp/kafka-logs/test3-0/00000000000000000000.log -print-data-log
```

12. 获取正在消费的topic的group的offset

```
./kafka-consumer-groups.sh --new-consumer --describe --group test6 --bootstrap-server hbp201:9092
```

13. 显示消费者

```
./kafka-consumer-groups.sh --bootstrap-server hdh56:9092,hdh57:9092,hdh58:9092 --list --new-consume
```

14. 消费的topic查看

```
./bin kafka-console-consumer.sh --topic __consumer_offsets --zookeeper localhost:2181 --formatter “kafka.coordinator.GroupMetadataManager$OffsetsMessageFormatter” --consumer.config /etc/KAFKA/consumer.properties --from-beginning
其中consumer.properties的group.id=消费的组，
exclude.internal.topics=false
```

15. kafka自带压测命令

```
./kafka-producer-perf-test.sh --topic test5 --num-records 100000 --record-size 1 --throughput 100 --producer-props bootstrap.servers=hbp001:9092
```

16. 平衡leader

```
./kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot
```

> **注意**
>
> –查看topic消费进度(不是consumer的offset)
> bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list node1:9092,node2:9092,node3:9092 --topic mytopic --time -1
> -1表示查询 mytopic 各个分区当前最大的消息位移值(注意，这里的位移不是consumer端的位移，而是指消息在每个分区的位置)
>
> 如果你要查询曾经生产过的最大消息数，那么只运行上面这条命令然后把各个分区的结果相加就可以了。
>
> 但如果你需要查询当前集群中该topic的消息数，那么还需要运行下面这条命令：
> bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list node1:9092,node2:9092,node3:9092 --topic mytopic --time -2
> -2表示去获取当前各个分区的最小位移。之后把运行第一条命令的结果与刚刚获取的位移之和相减就是集群中该topic的当前消息总数。

##### 7.5 Kafka Exactly Once 语义与事务机制原理

##### 7.5.1 why 事务机制

* Exactly Once即正好一次语义
* 操作的原子性
* 有状态操作的可恢复性

> **Exactly Once**
>
> 《Kafka背景及架构介绍》一文中有说明Kafka在0.11.0.0之前的版本中只支持`At Least Once`和`At Most Once`语义，尚不支持`Exactly Once`语义;
>
> 但是在很多要求严格的场景下，如使用Kafka处理交易数据，`Exactly Once`语义是必须的。我们可以通过让下游系统具有幂等性来配合Kafka的`At Least Once`语义来间接实现`Exactly Once`。但是;
>
> * 该方案要求下游系统支持幂等操作，限制了Kafka的适用场景
> * 实现门槛相对较高，需要用户对Kafka的工作机制非常了解
> * 对于Kafka Stream而言，Kafka本身即是自己的下游系统，但Kafka在0.11.0.0版本之前不具有幂等发送能力
>
> **操作原子性**
>
> 操作的原子性是指，多个操作要么全部成功要么全部失败，不存在部分成功部分失败的可能。
>
> 实现原子性操作的意义在于：
>
> * 操作结果更可控，有助于提升数据一致性
> * 便于故障恢复。因为操作是原子的，从故障中恢复时只需要重试该操作（如果原操作失败）或者直接跳过该操作（如果原操作成功），而不需要记录中间状态，更不需要针对中间状态作特殊处理

##### 7.5.2 实现事务机制的几个阶段

> **幂等性发送**
>
> 上文提到，实现`Exactly Once`的一种方法是让下游系统具有幂等处理特性，而在Kafka Stream中，Kafka Producer本身就是“下游”系统，因此如果能让Producer具有幂等处理特性，那就可以让Kafka Stream在一定程度上支持`Exactly once`语义；
>
> 为了实现Producer的幂等语义，Kafka引入了`Producer ID`（即`PID`）和`Sequence Number`。每个新的Producer在初始化的时候会被分配一个唯一的PID，该PID对用户完全透明而不会暴露给用户。
>
> 对于每个PID，该Producer发送数据的每个<Topic, Partition>都对应一个从0开始单调递增的`Sequence Number`。
>
> 类似地，Broker端也会为每个<PID, Topic, Partition>维护一个序号，并且每次Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号（即最后一次Commit的消息的序号）大一，则Broker会接受它，否则将其丢弃：
>
> * 如果消息序号比Broker维护的序号大一以上，说明中间有数据尚未写入，也即乱序，此时Broker拒绝该消息，Producer抛出InvalidSequenceNumber
> * 如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer抛出DuplicateSequenceNumber
>
> 上述设计解决了0.11.0.0之前版本中的两个问题：
>
> * Broker保存消息后，发送ACK前宕机，Producer认为消息未发送成功并重试，造成数据重复
> * 前一条消息发送失败，后一条消息发送成功，前一条消息重试后成功，造成数据乱序
>
> **事务性保证**
>
> 上述幂等设计只能保证单个Producer对于同一个<Topic, Partition>的`Exactly Once`语义
>
> 另外，它并不能保证写操作的原子性——即多个写操作，要么全部被Commit要么全部不被Commit。
>
> 更不能保证多个读写操作的的原子性。尤其对于Kafka Stream应用而言，典型的操作即是从某个Topic消费数据，经过一系列转换后写回另一个Topic，保证从源Topic的读取与向目标Topic的写入的原子性有助于从故障中恢复。
>
> 事务保证可使得应用程序将生产数据和消费数据当作一个原子单元来处理，要么全部成功，要么全部失败，即使该生产或消费跨多个<Topic, Partition>
>
> 另外，有状态的应用也可以保证重启后从断点处继续处理，也即事务恢复.
>
> 为了实现这种效果，应用程序必须提供一个稳定的（重启后不变）唯一的ID，也即`Transaction ID`。`Transactin ID`与`PID`可能一一对应。区别在于`Transaction ID`由用户提供，而`PID`是内部的实现对用户透明。
>
> 另外，为了保证新的Producer启动后，旧的具有相同`Transaction ID`的Producer即失效，每次Producer通过`Transaction ID`拿到PID的同时，还会获取一个单调递增的epoch。由于旧的Producer的epoch比新Producer的epoch小，Kafka可以很容易识别出该Producer是老的Producer并拒绝其请求。
>
> 有了`Transaction ID`后，Kafka可保证：
>
> * 跨Session的数据幂等发送。当具有相同Transaction ID的新的Producer实例被创建且工作时，旧的且拥有相同Transaction ID的Producer将不再工作
> * 跨Session的事务恢复。如果某个应用实例宕机，新的实例可以保证任何未完成的旧的事务要么Commit要么Abort，使得新实例从一个正常状态开始工作
>
> 需要注意的是，上述的事务保证是从Producer的角度去考虑的。从Consumer的角度来看，该保证会相对弱一些。尤其是不能保证所有被某事务Commit过的所有消息都被一起消费，因为
>
> * 对于压缩的Topic而言，同一事务的某些消息可能被其它版本覆盖
> * 事务包含的消息可能分布在多个Segment中（即使在同一个Partition内），当老的Segment被删除时，该事务的部分数据可能会丢失
> * Consumer在一个事务内可能通过seek方法访问任意Offset的消息，从而可能丢失部分消息
> * Consumer可能并不需要消费某一事务内的所有Partition，因此它将永远不会读取组成该事务的所有消息

##### 7.5.3事务机制原理

> **事务性消息传递**
>
> 这一节所说的事务主要指原子性，也即Producer将多条消息作为一个事务批量发送，要么全部成功要么全部失败。
>
> 为了实现这一点，Kafka 0.11.0.0引入了一个服务器端的模块，名为`Transaction Coordinator`，用于管理Producer发送的消息的事务性。
>
> 该`Transaction Coordinator`维护`Transaction Log`，该log存于一个内部的Topic内。由于Topic数据具有持久性，因此事务的状态也具有持久性。
>
> Producer并不直接读写`Transaction Log`，它与`Transaction Coordinator`通信，然后由`Transaction Coordinator`将该事务的状态插入相应的`Transaction Log`。
>
> `Transaction Log`的设计与`Offset Log`用于保存Consumer的Offset类似
>
> **事务中Offset的提交**
>
> 许多基于Kafka的应用，尤其是Kafka Stream应用中同时包含Consumer和Producer，前者负责从Kafka中获取消息，后者负责将处理完的数据写回Kafka的其它Topic中。
>
> 为了实现该场景下的事务的原子性，Kafka需要保证对Consumer Offset的Commit与Producer对发送消息的Commit包含在同一个事务中。否则，如果在二者Commit中间发生异常，根据二者Commit的顺序可能会造成数据丢失和数据重复：
>
> * 如果先Commit Producer发送数据的事务再Commit Consumer的Offset，即At Least Once语义，可能造成数据重复
> * 如果先Commit Consumer的Offset，再Commit Producer数据发送事务，即At Most Once语义，可能造成数据丢失
>
> **用于事务特性的控制型消息**
>
> 为了区分写入Partition的消息被Commit还是Abort，Kafka引入了一种特殊类型的消息，即`Control Message`。该类消息的Value内不包含任何应用相关的数据，并且不会暴露给应用程序。它只用于Broker与Client间的内部通信。
>
> 对于Producer端事务，Kafka以Control Message的形式引入一系列的`Transaction Marker`。Consumer即可通过该标记判定对应的消息被Commit了还是Abort了，然后结合该Consumer配置的隔离级别决定是否应该将该消息返回给应用程序。
>
> **事务处理样例代码**
>
> ```java
> Producer<String, String> producer = new KafkaProducer<String, String>(props);
>     
> // 初始化事务，包括结束该Transaction ID对应的未完成的事务（如果有）
> // 保证新的事务在一个正确的状态下启动
> producer.initTransactions();
> 
> // 开始事务
> producer.beginTransaction();
> 
> // 消费数据
> ConsumerRecords<String, String> records = consumer.poll(100);
> 
> try{
>    
>      
>     // 发送数据
>     producer.send(new ProducerRecord<String, String>("Topic", "Key", "Value"));
>     
>     // 发送消费数据的Offset，将上述数据消费与数据发送纳入同一个Transaction内
>     producer.sendOffsetsToTransaction(offsets, "group1");
> 
>     // 数据发送及Offset发送均成功的情况下，提交事务
>     producer.commitTransaction();
> } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
>    
>      
>     // 数据发送或者Offset发送出现异常时，终止事务
>     producer.abortTransaction();
> } finally {
>    
>      
>     // 关闭Producer和Consumer
>     producer.close();
>     consumer.close();
> }
> ```
>
> **完整事务过程**
>
> 1. #### 找到Transaction Coordinator
>
> 由于`Transaction Coordinator`是分配PID和管理事务的核心，因此Producer要做的第一件事情就是通过向任意一个Broker发送`FindCoordinator`请求找到`Transaction Coordinator`的位置。
>
> 注意：只有应用程序为Producer配置了`Transaction ID`时才可使用事务特性，也才需要这一步。另外，由于事务性要求Producer开启幂等特性，因此通过将`transactional.id`设置为非空从而开启事务特性的同时也需要通过将`enable.idempotence`设置为true来开启幂等特性。
>
> 2. #### 获取PID
>
> 找到`Transaction Coordinator`后，具有幂等特性的Producer必须发起`InitPidRequest`请求以获取PID。
>
> 注意：只要开启了幂等特性即必须执行该操作，而无须考虑该Producer是否开启了事务特性。
>
> ***如果事务特性被开启***
> `InitPidRequest`会发送给`Transaction Coordinator`。如果`Transaction Coordinator`是第一次收到包含有该`Transaction ID`的InitPidRequest请求，它将会把该``存入`Transaction Log`，如上图中步骤2.1所示。这样可保证该对应关系被持久化，从而保证即使`Transaction Coordinator`宕机该对应关系也不会丢失。
>
> 除了返回PID外，`InitPidRequest`还会执行如下任务:
>
> * 增加该PID对应的epoch。具有相同PID但epoch小于该epoch的其它Producer（如果有）新开启的事务将被拒绝
> * 恢复（Commit或Abort）之前的Producer未完成的事务（如果有）
>
> 注意：`InitPidRequest`的处理过程是同步阻塞的。一旦该调用正确返回，Producer即可开始新的事务。
>
> 另外，如果事务特性未开启，`InitPidRequest`可发送至任意Broker，并且会得到一个全新的唯一的PID。该Producer将只能使用幂等特性以及单一Session内的事务特性，而不能使用跨Session的事务特性。
>
> 3. #### 开启事务
>
> Kafka从0.11.0.0版本开始，提供`beginTransaction()`方法用于开启一个事务。调用该方法后，Producer本地会记录已经开启了事务，但`Transaction Coordinator`只有在Producer发送第一条消息后才认为事务已经开启。
>
> #### Consume-Transform-Produce
>
> 这一阶段，包含了整个事务的数据处理过程，并且包含了多种请求。
>
> ***AddPartitionsToTxnRequest***
> 一个Producer可能会给多个``发送数据，给一个新的``发送数据前，它需要先向`Transaction Coordinator`发送`AddPartitionsToTxnRequest`。
>
> `Transaction Coordinator`会将该``存于`Transaction Log`内，并将其状态置为`BEGIN`，如上图中步骤4.1所示。有了该信息后，我们才可以在后续步骤中为每个`Topic, Partition>`设置COMMIT或者ABORT标记（如上图中步骤5.2所示）。
>
> 另外，如果该``为该事务中第一个``，`Transaction Coordinator`还会启动对该事务的计时（每个事务都有自己的超时时间）。
>
> ***ProduceRequest***
> Producer通过一个或多个`ProduceRequest`发送一系列消息。除了应用数据外，该请求还包含了PID，epoch，和`Sequence Number`。该过程如上图中步骤4.2所示。
>
> ***AddOffsetsToTxnRequest***
> 为了提供事务性，Producer新增了`sendOffsetsToTransaction`方法，该方法将多组消息的发送和消费放入同一批处理内。
>
> 该方法先判断在当前事务中该方法是否已经被调用并传入了相同的Group ID。若是，直接跳到下一步；若不是，则向`Transaction Coordinator`发送`AddOffsetsToTxnRequests`请求，`Transaction Coordinator`将对应的所有``存于`Transaction Log`中，并将其状态记为`BEGIN`，如上图中步骤4.3所示。该方法会阻塞直到收到响应。
>
> ***TxnOffsetCommitRequest***
> 作为`sendOffsetsToTransaction`方法的一部分，在处理完`AddOffsetsToTxnRequest`后，Producer也会发送`TxnOffsetCommit`请求给`Consumer Coordinator`从而将本事务包含的与读操作相关的各``的Offset持久化到内部的`__consumer_offsets`中，如上图步骤4.4所示。
>
> 在此过程中，`Consumer Coordinator`会通过PID和对应的epoch来验证是否应该允许该Producer的该请求。
>
> 这里需要注意：
>
> * 写入__consumer_offsets的Offset信息在当前事务Commit前对外是不可见的。也即在当前事务被Commit前，可认为该Offset尚未Commit，也即对应的消息尚未被完成处理。
> * Consumer Coordinator并不会立即更新缓存中相应``的Offset，因为此时这些更新操作尚未被COMMIT或ABORT
>
> 4. #### Commit或Abort事务
>
> 一旦上述数据写入操作完成，应用程序必须调用`KafkaProducer`的`commitTransaction`方法或者`abortTransaction`方法以结束当前事务。
>
> ***EndTxnRequest***
> `commitTransaction`方法使得Producer写入的数据对下游Consumer可见。`abortTransaction`方法通过`Transaction Marker`将Producer写入的数据标记为`Aborted`状态。下游的Consumer如果将`isolation.level`设置为`READ_COMMITTED`，则它读到被Abort的消息后直接将其丢弃而不会返回给客户程序，也即被Abort的消息对应用程序不可见。
>
> 无论是Commit还是Abort，Producer都会发送`EndTxnRequest`请求给`Transaction Coordinator`，并通过标志位标识是应该Commit还是Abort。
>
> 收到该请求后，`Transaction Coordinator`会进行如下操作
>
> * 将`PREPARE_COMMIT`或`PREPARE_ABORT`消息写入`TransactionLog`，如上图中步骤5.1所示；
> * 通过`WriteTxnMarker`请求以`TransactionMarker`的形式将`COMMIT`或`ABORT`信息写入用户数据日志以及`OffsetLog`中，如上图中步骤5.2所示；
> * 最后将`COMPLETE_COMMIT`或`COMPLETE_ABORT`信息写入`TransactionLog`中，如上图中步骤5.3所示；
>
> 补充说明：对于`commitTransaction`方法，它会在发送`EndTxnRequest`之前先调用flush方法以确保所有发送出去的数据都得到相应的ACK。对于`abortTransaction`方法，在发送`EndTxnRequest`之前直接将当前Buffer中的事务性消息（如果有）全部丢弃，但必须等待所有被发送但尚未收到ACK的消息发送完成。
>
> 上述第二步是实现将一组读操作与写操作作为一个事务处理的关键。因为Producer写入的数据Topic以及记录Comsumer Offset的Topic会被写入相同的`Transactin Marker`，所以这一组读操作与写操作要么全部COMMIT要么全部ABORT。
>
> ***WriteTxnMarkerRequest***
> 上面提到的`WriteTxnMarkerRequest`由`Transaction Coordinator`发送给当前事务涉及到的每个``的Leader。收到该请求后，对应的Leader会将对应的`COMMIT(PID)`或者`ABORT(PID)`控制信息写入日志，如上图中步骤5.2所示。
>
> 该控制消息向Broker以及Consumer表明对应PID的消息被Commit了还是被Abort了。
>
> 这里要注意，如果事务也涉及到`__consumer_offsets`，即该事务中有消费数据的操作且将该消费的Offset存于`__consumer_offsets`中，`Transaction Coordinator`也需要向该内部Topic的各Partition的Leader发送`WriteTxnMarkerRequest`从而写入`COMMIT(PID)`或`COMMIT(PID)`控制信息。
>
> ***写入最终的`COMPLETE_COMMIT`或`COMPLETE_ABORT`消息***
> 写完所有的`Transaction Marker`后，`Transaction Coordinator`会将最终的`COMPLETE_COMMIT`或`COMPLETE_ABORT`消息写入`Transaction Log`中以标明该事务结束，如上图中步骤5.3所示。
>
> 此时，`Transaction Log`中所有关于该事务的消息全部可以移除。当然，由于Kafka内数据是Append Only的，不可直接更新和删除，这里说的移除只是将其标记为null从而在Log Compact时不再保留。
>
> 另外，`COMPLETE_COMMIT`或`COMPLETE_ABORT`的写入并不需要得到所有Rreplica的ACK，因为如果该消息丢失，可以根据事务协议重发。
>
> 补充说明，如果参与该事务的某些``在被写入`Transaction Marker`前不可用，它对`READ_COMMITTED`的Consumer不可见，但不影响其它可用``的COMMIT或ABORT。在该``恢复可用后，`Transaction Coordinator`会重新根据`PREPARE_COMMIT`或`PREPARE_ABORT`向该``发送`Transaction Marker`。
>
> **总结**
>
> * PID与Sequence Number的引入实现了写操作的幂等性
> * 写操作的幂等性结合At Least Once语义实现了单一Session内的Exactly Once语义
> * Transaction Marker与PID提供了识别消息是否应该被读取的能力，从而实现了事务的隔离性
> * Offset的更新标记了消息是否被读取，从而将对读操作的事务处理转换成了对写（Offset）操作的事务处理
> * Kafka事务的本质是，将一组写操作（如果有）对应的消息与一组读操作（如果有）对应的Offset的更新进行同样的标记（即Transaction Marker）来实现事务中涉及的所有读写操作同时对外可见或同时对外不可见
> * Kafka只提供对Kafka本身的读写操作的事务性，不提供包含外部系统的事务性

##### 7.5.4 异常处理

> ### Exception处理
>
> ***InvalidProducerEpoch***
> 这是一种Fatal Error，它说明当前Producer是一个过期的实例，有`Transaction ID`相同但epoch更新的Producer实例被创建并使用。此时Producer会停止并抛出Exception。
>
> ***InvalidPidMapping***
> `Transaction Coordinator`没有与该`Transaction ID`对应的PID。此时Producer会通过包含有`Transaction ID`的`InitPidRequest`请求创建一个新的PID。
>
> ***NotCorrdinatorForGTransactionalId***
> 该`Transaction Coordinator`不负责该当前事务。Producer会通过`FindCoordinatorRequest`请求重新寻找对应的`Transaction Coordinator`。
>
> ***InvalidTxnRequest***
> 违反了事务协议。正确的Client实现不应该出现这种Exception。如果该异常发生了，用户需要检查自己的客户端实现是否有问题。
>
> ***CoordinatorNotAvailable***
> `Transaction Coordinator`仍在初始化中。Producer只需要重试即可。
>
> ***DuplicateSequenceNumber***
> 发送的消息的序号低于Broker预期。该异常说明该消息已经被成功处理过，Producer可以直接忽略该异常并处理下一条消息
>
> ***InvalidSequenceNumber***
> 这是一个Fatal Error，它说明发送的消息中的序号大于Broker预期。此时有两种可能
>
> * 数据乱序。比如前面的消息发送失败后重试期间，新的消息被接收。正常情况下不应该出现该问题，因为当幂等发送启用时，max.inflight.requests.per.connection被强制设置为1，而acks被强制设置为all。故前面消息重试期间，后续消息不会被发送，也即不会发生乱序。并且只有ISR中所有Replica都ACK，Producer才会认为消息已经被发送，也即不存在Broker端数据丢失问题
> * 服务器由于日志被Truncate而造成数据丢失。此时应该停止Producer并将此Fatal Error报告给用户。
>
> ***InvalidTransactionTimeout***
> `InitPidRequest`调用出现的Fatal Error。它表明Producer传入的timeout时间不在可接受范围内，应该停止Producer并报告给用户
>
> ### 处理Transaction Coordinator失败
>
> #### 写PREPARE_COMMIT/PREPARE_ABORT前失败
>
> Producer通过`FindCoordinatorRequest`找到新的`Transaction Coordinator`，并通过`EndTxnRequest`请求发起`COMMIT`或`ABORT`流程，新的`Transaction Coordinator`继续处理`EndTxnRequest`请求——写`PREPARE_COMMIT`或`PREPARE_ABORT`，写`Transaction Marker`，写`COMPLETE_COMMIT`或`COMPLETE_ABORT`。
>
> #### 写完PREPARE_COMMIT/PREPARE_ABORT后失败
>
> 此时旧的`Transaction Coordinator`可能已经成功写入部分`Transaction Marker`。新的`Transaction Coordinator`会重复这些操作，所以部分Partition中可能会存在重复的`COMMIT`或`ABORT`，但只要该Producer在此期间没有发起新的事务，这些重复的`Transaction Marker`就不是问题。
>
> #### 写完COMPLETE_COMMIT/ABORT后失败
>
> 旧的`Transaction Coordinator`可能已经写完了`COMPLETE_COMMIT`或`COMPLETE_ABORT`但在返回`EndTxnRequest`之前失败。该场景下，新的`Transaction Coordinator`会直接给Producer返回成功

##### 7.5.5 事务过期机制

> #### 事务超时
>
> ```
> transaction.timeout.ms
> ```
>
> #### 终止过期事务
>
> 当Producer失败时，`Transaction Coordinator`必须能够主动的让某些进行中的事务过期。否则没有Producer的参与，`Transaction Coordinator`无法判断这些事务应该如何处理，这会造成：
>
> * 如果这种进行中事务太多，会造成Transaction Coordinator需要维护大量的事务状态，大量占用内存
> * Transaction Log内也会存在大量数据，造成新的Transaction Coordinator启动缓慢
> * READ_COMMITTED的Consumer需要缓存大量的消息，造成不必要的内存浪费甚至是OOM
> * 如果多个Transaction ID不同的Producer交叉写同一个Partition，当一个Producer的事务状态不更新时，READ_COMMITTED的Consumer为了保证顺序消费而被阻塞
>
> 为了避免上述问题，`Transaction Coordinator`会周期性遍历内存中的事务状态Map，并执行如下操作
>
> * 如果状态是BEGIN并且其最后更新时间与当前时间差大于transaction.remove.expired.transaction.cleanup.interval.ms（默认值为1小时），则主动将其终止：1）未避免原Producer临时恢复与当前终止流程冲突，增加该Producer对应的PID的epoch，并确保将该更新的信息写入Transaction Log；2）以更新后的epoch回滚事务，从而使得该事务相关的所有Broker都更新其缓存的该PID的epoch从而拒绝旧Producer的写操作
> * 如果状态是PREPARE_COMMIT，完成后续的COMMIT流程————向各``写入Transaction Marker，在Transaction Log内写入COMPLETE_COMMIT
> * 如果状态是PREPARE_ABORT，完成后续ABORT流程
>
> #### 终止Transaction ID
>
> 某`Transaction ID`的Producer可能很长时间不再发送数据，`Transaction Coordinator`没必要再保存该`Transaction ID`与`PID`等的映射，否则可能会造成大量的资源浪费。因此需要有一个机制探测不再活跃的`Transaction ID`并将其信息删除。
>
> `Transaction Coordinator`会周期性遍历内存中的`Transaction ID`与`PID`映射，如果某`Transaction ID`没有对应的正在进行中的事务并且它对应的最后一个事务的结束时间与当前时间差大于`transactional.id.expiration.ms`（默认值是7天），则将其从内存中删除并在`Transaction Log`中将其对应的日志的值设置为null从而使得Log Compact可将其记录删除

##### 7.6 kafka消费者组subscribe和assign的正确使用

> 使用Apache Kafka 消费者组时，有一个为消费者分配对应分区partition的过程，我们可以使用“自动”subscribe和“手动”assign的方式。
>
> 同时进行“自动”和“手动”的分区分配是会互相影响的，有时会把事情搞糟。正确的使用，首先要了解这两种方式的场景。

###### 7.6.1 消费者组的使用场景

Kafka里的消费者组有两个使用的场景：

* “队列模式”：在同一组的消费者共同消费一个主题的所有消息，而且确保一条消息只被一个消费者处理。一个主题的所有的分区会和一个消费组的所有消费者做关联：每个消费者和一到多个分区做关联，接收它们的消息。反向说，一个分区只会与一个消费者关联，它的消息不会被其它的消费者接收。
  最开始只有一个消费者时，所有的分区都分配给了它。当消息的规模增加时，我们就需要扩展消费者的数量，水平扩展处理能力，一直可以达到每个消费者只关联一个分区。大于分区数的消费者是会处在空闲状态，因为没有分配任何的分区
* “发布/订阅模式” 创建不同的消费者组意味一个主题的消息会发送给所有订阅它的消费者组，然后消费者组依照前面共同协作的场景进行分配。这往往是因为我们有不同的应用需求，比如一批交易数据，资金系统、ERP系统会消费它而风险监控也需要同时消费它。这就实现了数据的透明异步共用。
  在两个场景中，消费者组有个重要的功能：rebalancing。当一个新的消费者加入一个组，如果还有有效的分区（消费者数<=主题分区数），会开始一个重新均衡分配的操作，会将一个已关联的分区（它的原消费者仍保有至少一个分区）重新分配给新加入的消费者。同样的，当一个消费者因为各种原因离开这个组，它的所有分区会被分配给剩下的消费者

###### 7.6.2 自动 OR 手动

前面所说的自动分配是指在 KafkaConsumer API中的subscribe()方法。这个方法强制要求你为消费者设置一个消费者组，group.id参数不能为空。而你不需要处理分区的分配问题。
而对应subscribe()方法，你可以采用手动的方式，指定消费者读取哪个主题分区，则：assign() 方法。当你需要精确地控制消息处理的负载，也能确定哪个分区有哪些消息时，这种手动的方式会很有用。但这时Kafka也无法提供rebalancing的功能了。而且在使用手动的方式时，你可以不指定消费者组，group.id为空。
两种方式都各有适用场景，但同时不建议同时使用两种方式，这会带来风险。假设一个消费者组G1，组内只有一个消费者C1，订阅subscribe了一个具有两个分区P1、P2的主题T1。这时在G1新增一个消费者C2，用assign的方式关联P1和P2。视乎一切都可行，但其实是糟糕的情况。本质上，使用场景被混淆了，你无法确定G1是在共同协助还是在进行发布/订阅。实际使用中，offset的提交格式是这样的：

```
key = [group, topic, partition]
value = offset
```

注意Key中并未区分消费者，C1和C2会对同一个key会脏写。代表着C1或C2奔溃重启时可能会拿到对方重写覆盖的offset，消息也会有丢失。